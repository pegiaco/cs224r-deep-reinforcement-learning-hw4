Env ID: [22]
================================================================================
Timestep: 0
State: tensor([2, 2, 0, 0])
Action: noop
Reward: -0.10023623704910278
Distance: 0.27743396162986755
Next state: tensor([2, 2, 0, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([2, 2, 0, 0])
Action: left
Reward: -0.10480502247810364
Distance: 0.28085893392562866
Next state: tensor([1, 2, 1, 0])
================================================================================

================================================================================
Timestep: 2
State: tensor([1, 2, 1, 0])
Action: left
Reward: -0.1085343062877655
Distance: 0.28140416741371155
Next state: tensor([0, 2, 0, 0])
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 2, 0, 0])
Action: pickup
Reward: -0.09444022923707962
Distance: 0.2853870689868927
Next state: tensor([0, 2, 0, 0])
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 2, 0, 0])
Action: drop
Reward: -0.054959602653980255
Distance: 0.2776271402835846
Next state: tensor([0, 2, 0, 0])
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 2, 0, 0])
Action: down
Reward: -0.1506512463092804
Distance: 0.2805800437927246
Next state: tensor([0, 1, 0, 0])
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 0, 0])
Action: left
Reward: -0.0992976725101471
Distance: 0.28649452328681946
Next state: tensor([0, 1, 0, 0])
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 0, 0])
Action: end_episode
Reward: -0.09116707742214203
Distance: 0.285930335521698
Next state: tensor([0, 1, 0, 0])
================================================================================

